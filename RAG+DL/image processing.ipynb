{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8EmH4T24vXCQqdEIDmiid"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6f0ba69b978a4f808bd272e8d4bed8d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dab9aa41ea3c4536a6d1bea495227b61","IPY_MODEL_62054135d9fd4fa6b64ae7b93ba9216d","IPY_MODEL_ef0036967cb943e3a0838489f60e88b6"],"layout":"IPY_MODEL_7d9eb1eded7941be9b3254b785bc2da6"}},"dab9aa41ea3c4536a6d1bea495227b61":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88d333ed31e5465aac7b568f00a97f11","placeholder":"‚Äã","style":"IPY_MODEL_07ab9d29d54c405096ebfd50c1629544","value":"Loading‚Äáweights:‚Äá100%"}},"62054135d9fd4fa6b64ae7b93ba9216d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1408523bc5794af292bff46a317f90e7","max":616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15a539e592a7419c9d1d6c9da05717c5","value":616}},"ef0036967cb943e3a0838489f60e88b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b597d31cc3e43fb9f41582780718d98","placeholder":"‚Äã","style":"IPY_MODEL_0dd5a8245dc84248ae57cd8a08634991","value":"‚Äá616/616‚Äá[00:01&lt;00:00,‚Äá697.63it/s,‚ÄáMaterializing‚Äáparam=vision_model.post_layernorm.weight]"}},"7d9eb1eded7941be9b3254b785bc2da6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88d333ed31e5465aac7b568f00a97f11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07ab9d29d54c405096ebfd50c1629544":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1408523bc5794af292bff46a317f90e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15a539e592a7419c9d1d6c9da05717c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3b597d31cc3e43fb9f41582780718d98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dd5a8245dc84248ae57cd8a08634991":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2aa1cd544c334109a61883e9e0a2c918":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a49adef1c384d458622abe139906477","IPY_MODEL_591cbec74c5a4a4390ea184a48beec55","IPY_MODEL_6dcbae673e4a41f892df903b2a4c6ce6"],"layout":"IPY_MODEL_50756097091649db807705b6262be366"}},"3a49adef1c384d458622abe139906477":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4933adcf92d342cc83f40973dc250259","placeholder":"‚Äã","style":"IPY_MODEL_a2327996c9f84291baf320399b99ab59","value":"Loading‚Äáweights:‚Äá100%"}},"591cbec74c5a4a4390ea184a48beec55":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1e6ba9455024b87a05be8349841daa9","max":472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3718da1d29d64609a836af8a4bab2535","value":472}},"6dcbae673e4a41f892df903b2a4c6ce6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4ac5625cc35424eb2538ecc0f8a5c8f","placeholder":"‚Äã","style":"IPY_MODEL_d6fea5342ea14af7bce99eeeaabe470d","value":"‚Äá472/472‚Äá[00:00&lt;00:00,‚Äá662.73it/s,‚ÄáMaterializing‚Äáparam=vision_model.post_layernorm.weight]"}},"50756097091649db807705b6262be366":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4933adcf92d342cc83f40973dc250259":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2327996c9f84291baf320399b99ab59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1e6ba9455024b87a05be8349841daa9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3718da1d29d64609a836af8a4bab2535":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b4ac5625cc35424eb2538ecc0f8a5c8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6fea5342ea14af7bce99eeeaabe470d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be671b7633654974a8c7435e7dc2a050":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8913207a4cca4fceafb7602c839d516a","IPY_MODEL_6a3d3a8a7e8b40fd9a0d92a30f31ec79","IPY_MODEL_2b62dd43d2844195bbdb603374e31d5c"],"layout":"IPY_MODEL_e866e1f1f0684f309034402ab7d57278"}},"8913207a4cca4fceafb7602c839d516a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_036ff12cd79245d4a5c3d3adaef9ecdf","placeholder":"‚Äã","style":"IPY_MODEL_1d639b1dcfa349d6ab7c378c1165f429","value":"Loading‚Äáweights:‚Äá100%"}},"6a3d3a8a7e8b40fd9a0d92a30f31ec79":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d0f6e7436eb4bee825545202f010be6","max":515,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8614d1e16e4e427eb8bbdc523ad7ab62","value":515}},"2b62dd43d2844195bbdb603374e31d5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_501473c2337244f694537de5dbb8bc01","placeholder":"‚Äã","style":"IPY_MODEL_a1d8ade8cbef48cca3a96b2b9309735a","value":"‚Äá515/515‚Äá[00:00&lt;00:00,‚Äá673.65it/s,‚ÄáMaterializing‚Äáparam=model.shared.weight]"}},"e866e1f1f0684f309034402ab7d57278":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"036ff12cd79245d4a5c3d3adaef9ecdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d639b1dcfa349d6ab7c378c1165f429":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d0f6e7436eb4bee825545202f010be6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8614d1e16e4e427eb8bbdc523ad7ab62":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"501473c2337244f694537de5dbb8bc01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1d8ade8cbef48cca3a96b2b9309735a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygvRI_tveGAO","executionInfo":{"status":"ok","timestamp":1770323006702,"user_tz":-360,"elapsed":15936,"user":{"displayName":"Maharuj Mahadi","userId":"06320750487425513154"}},"outputId":"436f7638-4fae-43c0-dea2-3a63122722d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.8)\n","Collecting langchain-community\n","  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n","Collecting pypdf\n","  Downloading pypdf-6.6.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n","Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n","Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.2)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n","Requirement already satisfied: langchain-core<2.0.0,>=1.2.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.8)\n","Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.7)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n","Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n","  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.46)\n","Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n","  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n","Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n","Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.8)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n","  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n","Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n","Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n","  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (1.33)\n","Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.14.0)\n","Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n","Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n","Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n","Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.7)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.8->langchain) (3.0.0)\n","Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-6.6.2-py3-none-any.whl (329 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n","Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: requests, pypdf, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, dataclasses-json, langchain-text-splitters, langchain-classic, langchain-community\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.32.4\n","    Uninstalling requests-2.32.4:\n","      Successfully uninstalled requests-2.32.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.13.2 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-text-splitters-1.1.0 marshmallow-3.26.2 mypy-extensions-1.1.0 pypdf-6.6.2 requests-2.32.5 typing-inspect-0.9.0\n"]}],"source":["!pip install transformers torch pillow sentence-transformers langchain langchain-community pypdf faiss-cpu"]},{"cell_type":"code","source":["from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n","from PIL import Image\n","import torch\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_community.vectorstores import FAISS\n","\n","class FreeStructuralDefectAnalyzer:\n","    def __init__(self, pdf_path=None):\n","        self.pdf_path = pdf_path\n","        self.vectorstore = None\n","        self.image_captioner = None\n","        self.vqa_model = None\n","        self.classifier = None\n","\n","    def setup(self):\n","        \"\"\"Setup all models\"\"\"\n","        print(\"=\" * 80)\n","        print(\"INITIALIZING FREE STRUCTURAL ANALYSIS SYSTEM\")\n","        print(\"=\" * 80)\n","\n","        # 1. Image Captioning Model\n","        print(\"\\n[1/4] Loading image captioning model (BLIP)...\")\n","        self.blip_processor = BlipProcessor.from_pretrained(\n","            \"Salesforce/blip-image-captioning-large\"\n","        )\n","        self.blip_model = BlipForConditionalGeneration.from_pretrained(\n","            \"Salesforce/blip-image-captioning-large\"\n","        )\n","        print(\"      ‚úì BLIP model loaded\")\n","\n","        # 2. Visual Question Answering Model\n","        print(\"\\n[2/4] Loading Visual QA model (BLIP-VQA)...\")\n","        self.vqa_processor = BlipProcessor.from_pretrained(\n","            \"Salesforce/blip-vqa-base\"\n","        )\n","        self.vqa_model = BlipForConditionalGeneration.from_pretrained(\n","            \"Salesforce/blip-vqa-base\"\n","        )\n","        print(\"      ‚úì VQA model loaded\")\n","\n","        # 3. Text Classification for defect analysis\n","        print(\"\\n[3/4] Loading zero-shot classifier...\")\n","        self.classifier = pipeline(\n","            \"zero-shot-classification\",\n","            model=\"facebook/bart-large-mnli\"\n","        )\n","        print(\"      ‚úì Classifier loaded\")\n","\n","        # 4. Load PDF guidelines (optional)\n","        if self.pdf_path:\n","            print(\"\\n[4/4] Loading building guidelines PDF...\")\n","            loader = PyPDFLoader(self.pdf_path)\n","            documents = loader.load()\n","\n","            text_splitter = RecursiveCharacterTextSplitter(\n","                chunk_size=1000,\n","                chunk_overlap=200\n","            )\n","            texts = text_splitter.split_documents(documents)\n","\n","            embeddings = HuggingFaceEmbeddings(\n","                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n","            )\n","            self.vectorstore = FAISS.from_documents(texts, embeddings)\n","            print(f\"      ‚úì Loaded {len(documents)} pages from PDF\")\n","\n","        print(\"\\n\" + \"=\" * 80)\n","        print(\"‚úÖ SYSTEM READY!\")\n","        print(\"=\" * 80 + \"\\n\")\n","\n","    def analyze_image(self, image_path):\n","        \"\"\"Generate detailed image description\"\"\"\n","        image = Image.open(image_path).convert('RGB')\n","\n","        # General caption\n","        inputs = self.blip_processor(image, return_tensors=\"pt\")\n","        out = self.blip_model.generate(**inputs, max_length=100)\n","        general_caption = self.blip_processor.decode(out[0], skip_special_tokens=True)\n","\n","        # Detailed technical caption\n","        text_prompt = \"a detailed view of\"\n","        inputs = self.blip_processor(image, text_prompt, return_tensors=\"pt\")\n","        out = self.blip_model.generate(**inputs, max_length=100)\n","        detailed_caption = self.blip_processor.decode(out[0], skip_special_tokens=True)\n","\n","        return general_caption, detailed_caption\n","\n","    def ask_visual_questions(self, image_path):\n","        \"\"\"Ask specific questions about the image\"\"\"\n","        image = Image.open(image_path).convert('RGB')\n","\n","        questions = [\n","            \"Is this a crack?\",\n","            \"Is the crack diagonal or horizontal?\",\n","            \"Is there exposed rebar?\",\n","            \"Is there concrete spalling?\",\n","            \"Is this on a column or beam?\",\n","            \"Is the crack wide?\",\n","            \"Is this structural damage?\"\n","        ]\n","\n","        answers = {}\n","        for question in questions:\n","            inputs = self.vqa_processor(image, question, return_tensors=\"pt\")\n","            out = self.vqa_model.generate(**inputs, max_length=20)\n","            answer = self.vqa_processor.decode(out[0], skip_special_tokens=True)\n","            answers[question] = answer\n","\n","        return answers\n","\n","    def classify_defect(self, description, vqa_answers):\n","        \"\"\"Classify as structural or cosmetic\"\"\"\n","\n","        # Combine all information\n","        full_description = f\"{description}. \"\n","        for q, a in vqa_answers.items():\n","            full_description += f\"{q} {a}. \"\n","\n","        # Define categories\n","        candidate_labels = [\n","            \"structural shear crack\",\n","            \"structural flexural crack\",\n","            \"structural column damage\",\n","            \"concrete spalling with rebar exposure\",\n","            \"short column defect\",\n","            \"cosmetic plaster crack\",\n","            \"cosmetic paint damage\",\n","            \"cosmetic surface issue\"\n","        ]\n","\n","        # Classify\n","        result = self.classifier(\n","            full_description,\n","            candidate_labels,\n","            multi_label=False\n","        )\n","\n","        # Determine if structural or cosmetic\n","        top_label = result['labels'][0]\n","        top_score = result['scores'][0]\n","\n","        is_structural = 'structural' in top_label.lower()\n","        classification = \"STRUCTURAL\" if is_structural else \"COSMETIC\"\n","\n","        # Severity based on keywords\n","        severity = \"LOW\"\n","        if is_structural:\n","            if any(kw in full_description.lower() for kw in ['rebar', 'spalling', 'wide', 'column']):\n","                severity = \"CRITICAL\"\n","            elif 'shear' in top_label or 'diagonal' in full_description.lower():\n","                severity = \"HIGH\"\n","            else:\n","                severity = \"MEDIUM\"\n","\n","        return {\n","            'classification': classification,\n","            'predicted_defect': top_label,\n","            'confidence': top_score,\n","            'severity': severity,\n","            'all_predictions': list(zip(result['labels'], result['scores']))\n","        }\n","\n","    def analyze_structural_defect(self, image_path):\n","        \"\"\"Complete structural defect analysis\"\"\"\n","        print(\"\\n\" + \"=\" * 80)\n","        print(\"STRUCTURAL DEFECT ANALYSIS\")\n","        print(\"=\" * 80)\n","        print(f\"Image: {image_path}\\n\")\n","\n","        # Step 1: Analyze image\n","        print(\"üîç Step 1: Analyzing image...\")\n","        general, detailed = self.analyze_image(image_path)\n","        print(f\"   General: {general}\")\n","        print(f\"   Detailed: {detailed}\")\n","\n","        # Step 2: Visual Q&A\n","        print(\"\\nüîç Step 2: Visual question answering...\")\n","        vqa_answers = self.ask_visual_questions(image_path)\n","        for q, a in vqa_answers.items():\n","            print(f\"   Q: {q}\")\n","            print(f\"   A: {a}\")\n","\n","        # Step 3: Classification\n","        print(\"\\nüîç Step 3: Classifying defect type...\")\n","        classification = self.classify_defect(detailed, vqa_answers)\n","\n","        # Display results\n","        print(\"\\n\" + \"=\" * 80)\n","        print(\"üìä ANALYSIS RESULTS\")\n","        print(\"=\" * 80)\n","        print(f\"\\nüè∑Ô∏è  CLASSIFICATION: {classification['classification']}\")\n","        print(f\"‚ö†Ô∏è  SEVERITY LEVEL: {classification['severity']}\")\n","        print(f\"üéØ PREDICTED DEFECT: {classification['predicted_defect']}\")\n","        print(f\"üìà CONFIDENCE: {classification['confidence']:.2%}\")\n","\n","        print(\"\\nüìã DETAILED PREDICTIONS:\")\n","        print(\"-\" * 80)\n","        for label, score in classification['all_predictions'][:5]:\n","            indicator = \"üî¥\" if \"structural\" in label.lower() else \"üü¢\"\n","            print(f\"{indicator} {score:.2%} - {label}\")\n","\n","        # Recommendations\n","        print(\"\\nüí° RECOMMENDATIONS:\")\n","        print(\"-\" * 80)\n","        if classification['classification'] == \"STRUCTURAL\":\n","            if classification['severity'] == \"CRITICAL\":\n","                print(\"‚ö†Ô∏è  URGENT ACTION REQUIRED:\")\n","                print(\"   - Immediate structural engineer inspection\")\n","                print(\"   - Restrict access to affected area\")\n","                print(\"   - Document and monitor closely\")\n","                print(\"   - Consider temporary shoring\")\n","            elif classification['severity'] == \"HIGH\":\n","                print(\"‚ö†Ô∏è  PRIORITY REPAIR NEEDED:\")\n","                print(\"   - Schedule structural engineer inspection within 48 hours\")\n","                print(\"   - Monitor for progression\")\n","                print(\"   - Avoid heavy loading in affected area\")\n","            else:\n","                print(\"‚ö†Ô∏è  INSPECTION RECOMMENDED:\")\n","                print(\"   - Schedule structural assessment\")\n","                print(\"   - Monitor and document\")\n","                print(\"   - Plan for repair\")\n","        else:\n","            print(\"‚úÖ COSMETIC ISSUE:\")\n","            print(\"   - Schedule routine maintenance\")\n","            print(\"   - Cosmetic repair sufficient\")\n","            print(\"   - Continue normal monitoring\")\n","\n","        # Building code references\n","        if self.vectorstore:\n","            print(\"\\nüìö RELEVANT BUILDING CODE SECTIONS:\")\n","            print(\"=\" * 80)\n","            search_query = f\"{classification['predicted_defect']} crack repair structural requirements\"\n","            docs = self.vectorstore.similarity_search(search_query, k=2)\n","            for i, doc in enumerate(docs, 1):\n","                print(f\"\\n[Reference {i} - Page {doc.metadata.get('page', 'N/A')}]\")\n","                print(\"-\" * 80)\n","                print(doc.page_content[:500] + \"...\")\n","\n","        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n","\n","        return classification\n","\n","    def batch_analyze(self, image_paths):\n","        \"\"\"Analyze multiple images\"\"\"\n","        results = []\n","\n","        for i, img_path in enumerate(image_paths, 1):\n","            print(f\"\\n{'#'*80}\")\n","            print(f\"IMAGE {i}/{len(image_paths)}\")\n","            print(f\"{'#'*80}\")\n","\n","            result = self.analyze_structural_defect(img_path)\n","            results.append({\n","                'image': img_path,\n","                'result': result\n","            })\n","\n","        # Summary report\n","        print(\"\\n\" + \"=\" * 80)\n","        print(\"BATCH ANALYSIS SUMMARY REPORT\")\n","        print(\"=\" * 80)\n","\n","        structural = [r for r in results if r['result']['classification'] == 'STRUCTURAL']\n","        cosmetic = [r for r in results if r['result']['classification'] == 'COSMETIC']\n","\n","        critical = [r for r in structural if r['result']['severity'] == 'CRITICAL']\n","        high = [r for r in structural if r['result']['severity'] == 'HIGH']\n","\n","        print(f\"\\nüìä OVERVIEW:\")\n","        print(f\"   Total Images Analyzed: {len(results)}\")\n","        print(f\"   Structural Issues: {len(structural)}\")\n","        print(f\"   Cosmetic Issues: {len(cosmetic)}\")\n","\n","        print(f\"\\n‚ö†Ô∏è  SEVERITY BREAKDOWN:\")\n","        print(f\"   Critical: {len(critical)}\")\n","        print(f\"   High: {len(high)}\")\n","        print(f\"   Medium/Low: {len(structural) - len(critical) - len(high)}\")\n","\n","        if critical:\n","            print(f\"\\nüö® CRITICAL ISSUES REQUIRING IMMEDIATE ATTENTION:\")\n","            for r in critical:\n","                print(f\"   - {r['image']}: {r['result']['predicted_defect']}\")\n","\n","        if high:\n","            print(f\"\\n‚ö†Ô∏è  HIGH PRIORITY ISSUES:\")\n","            for r in high:\n","                print(f\"   - {r['image']}: {r['result']['predicted_defect']}\")\n","\n","        print(\"\\n\" + \"=\" * 80 + \"\\n\")\n","\n","        return results\n","\n","# ============================================================================\n","# USAGE EXAMPLES\n","# ============================================================================\n","\n","# Initialize the system\n","analyzer = FreeStructuralDefectAnalyzer(\n","    pdf_path=None  # Optional, set to None to skip PDF loading if file is not available\n",")\n","analyzer.setup()\n","\n","# Example 1: Analyze single image\n","# Make sure to replace '/content/crack_photo.jpg' with a valid image path or upload the image.\n","# For now, we'll comment out the analysis to prevent further file-not-found errors if images are missing.\n","# result = analyzer.analyze_structural_defect(\"/content/crack_photo.jpg\")\n","\n","# Example 2: Batch analysis\n","# Make sure to replace '/content/crack1.jpg', etc., with valid image paths or upload the images.\n","# For now, we'll comment out the analysis to prevent further file-not-found errors if images are missing.\n","# results = analyzer.batch_analyze([\n","#     \"/content/crack1.jpg\",\n","#     \"/content/crack2.jpg\",\n","#     \"/content/crack3.jpg\"\n","# ])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6f0ba69b978a4f808bd272e8d4bed8d1","dab9aa41ea3c4536a6d1bea495227b61","62054135d9fd4fa6b64ae7b93ba9216d","ef0036967cb943e3a0838489f60e88b6","7d9eb1eded7941be9b3254b785bc2da6","88d333ed31e5465aac7b568f00a97f11","07ab9d29d54c405096ebfd50c1629544","1408523bc5794af292bff46a317f90e7","15a539e592a7419c9d1d6c9da05717c5","3b597d31cc3e43fb9f41582780718d98","0dd5a8245dc84248ae57cd8a08634991","2aa1cd544c334109a61883e9e0a2c918","3a49adef1c384d458622abe139906477","591cbec74c5a4a4390ea184a48beec55","6dcbae673e4a41f892df903b2a4c6ce6","50756097091649db807705b6262be366","4933adcf92d342cc83f40973dc250259","a2327996c9f84291baf320399b99ab59","b1e6ba9455024b87a05be8349841daa9","3718da1d29d64609a836af8a4bab2535","b4ac5625cc35424eb2538ecc0f8a5c8f","d6fea5342ea14af7bce99eeeaabe470d","be671b7633654974a8c7435e7dc2a050","8913207a4cca4fceafb7602c839d516a","6a3d3a8a7e8b40fd9a0d92a30f31ec79","2b62dd43d2844195bbdb603374e31d5c","e866e1f1f0684f309034402ab7d57278","036ff12cd79245d4a5c3d3adaef9ecdf","1d639b1dcfa349d6ab7c378c1165f429","6d0f6e7436eb4bee825545202f010be6","8614d1e16e4e427eb8bbdc523ad7ab62","501473c2337244f694537de5dbb8bc01","a1d8ade8cbef48cca3a96b2b9309735a"]},"id":"xO6jfHZleU06","executionInfo":{"status":"ok","timestamp":1770323400701,"user_tz":-360,"elapsed":8764,"user":{"displayName":"Maharuj Mahadi","userId":"06320750487425513154"}},"outputId":"9d3b3f2d-c0fd-4efb-a0c9-acabb51c3d52"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","INITIALIZING FREE STRUCTURAL ANALYSIS SYSTEM\n","================================================================================\n","\n","[1/4] Loading image captioning model (BLIP)...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading weights:   0%|          | 0/616 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f0ba69b978a4f808bd272e8d4bed8d1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tied weights mapping and config for this model specifies to tie text_decoder.bert.embeddings.word_embeddings.weight to text_decoder.cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n","BlipForConditionalGeneration LOAD REPORT from: Salesforce/blip-image-captioning-large\n","Key                                       | Status     |  | \n","------------------------------------------+------------+--+-\n","text_decoder.bert.embeddings.position_ids | UNEXPECTED |  | \n","\n","Notes:\n","- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"]},{"output_type":"stream","name":"stdout","text":["      ‚úì BLIP model loaded\n","\n","[2/4] Loading Visual QA model (BLIP-VQA)...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading weights:   0%|          | 0/472 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aa1cd544c334109a61883e9e0a2c918"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tied weights mapping and config for this model specifies to tie text_decoder.bert.embeddings.word_embeddings.weight to text_decoder.cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n","BlipForConditionalGeneration LOAD REPORT from: Salesforce/blip-vqa-base\n","Key                                                                        | Status     |  | \n","---------------------------------------------------------------------------+------------+--+-\n","text_encoder.encoder.layer.{0...11}.output.LayerNorm.bias                  | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.output.dense.weight                    | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.output.dense.bias                      | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.attention.self.value.bias              | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.intermediate.dense.bias                | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.attention.output.dense.bias            | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.crossattention.self.value.bias         | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.crossattention.output.dense.weight     | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.attention.self.query.weight            | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.output.LayerNorm.weight                | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.attention.self.query.bias              | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.intermediate.dense.weight              | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.crossattention.output.LayerNorm.bias   | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.crossattention.output.dense.bias       | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.attention.self.key.weight              | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.attention.self.key.bias                | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.attention.output.dense.weight          | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.crossattention.output.LayerNorm.weight | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.crossattention.self.key.weight         | UNEXPECTED |  | \n","text_encoder.embeddings.position_ids                                       | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.crossattention.self.query.bias         | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.crossattention.self.query.weight       | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.attention.output.LayerNorm.weight      | UNEXPECTED |  | \n","text_encoder.embeddings.LayerNorm.weight                                   | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.attention.output.LayerNorm.bias        | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.crossattention.self.key.bias           | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.crossattention.self.value.weight       | UNEXPECTED |  | \n","text_encoder.embeddings.LayerNorm.bias                                     | UNEXPECTED |  | \n","text_encoder.encoder.layer.{0...11}.attention.self.value.weight            | UNEXPECTED |  | \n","text_encoder.embeddings.position_embeddings.weight                         | UNEXPECTED |  | \n","text_decoder.bert.embeddings.position_ids                                  | UNEXPECTED |  | \n","text_encoder.embeddings.word_embeddings.weight                             | UNEXPECTED |  | \n","\n","Notes:\n","- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"]},{"output_type":"stream","name":"stdout","text":["      ‚úì VQA model loaded\n","\n","[3/4] Loading zero-shot classifier...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading weights:   0%|          | 0/515 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be671b7633654974a8c7435e7dc2a050"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["      ‚úì Classifier loaded\n","\n","================================================================================\n","‚úÖ SYSTEM READY!\n","================================================================================\n","\n"]}]},{"cell_type":"code","source":["# Upload images in Colab\n","from google.colab import files\n","\n","print(\"üì∏ Upload your crack/defect images:\")\n","uploaded = files.upload()\n","\n","# Analyze all uploaded images\n","image_paths = list(uploaded.keys())\n","analyzer.batch_analyze(image_paths)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"oeX7XQpwf_sF","executionInfo":{"status":"ok","timestamp":1770323525608,"user_tz":-360,"elapsed":80381,"user":{"displayName":"Maharuj Mahadi","userId":"06320750487425513154"}},"outputId":"f4225539-3f91-45de-94e8-bb8cfbaa902f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["üì∏ Upload your crack/defect images:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-3699560a-8ec9-4743-af09-b01fb12c1ac1\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3699560a-8ec9-4743-af09-b01fb12c1ac1\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving beam crack 01.jfif to beam crack 01.jfif\n","\n","################################################################################\n","IMAGE 1/1\n","################################################################################\n","\n","================================================================================\n","STRUCTURAL DEFECT ANALYSIS\n","================================================================================\n","Image: beam crack 01.jfif\n","\n","üîç Step 1: Analyzing image...\n","   General: a close up of a crack in the wall of a bathroom\n","   Detailed: a detailed view of a crack in the floor of a bathroom\n","\n","üîç Step 2: Visual question answering...\n","   Q: Is this a crack?\n","   A: is this a crack?\n","   Q: Is the crack diagonal or horizontal?\n","   A: is the crack diagonal or horizontal?\n","   Q: Is there exposed rebar?\n","   A: is there exposed rebar?\n","   Q: Is there concrete spalling?\n","   A: is there concrete spalling?\n","   Q: Is this on a column or beam?\n","   A: is this on a column or beam?\n","   Q: Is the crack wide?\n","   A: is the crack wide?\n","   Q: Is this structural damage?\n","   A: is this structural damage?\n","\n","üîç Step 3: Classifying defect type...\n","\n","================================================================================\n","üìä ANALYSIS RESULTS\n","================================================================================\n","\n","üè∑Ô∏è  CLASSIFICATION: COSMETIC\n","‚ö†Ô∏è  SEVERITY LEVEL: LOW\n","üéØ PREDICTED DEFECT: concrete spalling with rebar exposure\n","üìà CONFIDENCE: 71.12%\n","\n","üìã DETAILED PREDICTIONS:\n","--------------------------------------------------------------------------------\n","üü¢ 71.12% - concrete spalling with rebar exposure\n","üî¥ 13.00% - structural column damage\n","üî¥ 5.40% - structural flexural crack\n","üî¥ 4.05% - structural shear crack\n","üü¢ 3.63% - short column defect\n","\n","üí° RECOMMENDATIONS:\n","--------------------------------------------------------------------------------\n","‚úÖ COSMETIC ISSUE:\n","   - Schedule routine maintenance\n","   - Cosmetic repair sufficient\n","   - Continue normal monitoring\n","\n","================================================================================\n","\n","\n","================================================================================\n","BATCH ANALYSIS SUMMARY REPORT\n","================================================================================\n","\n","üìä OVERVIEW:\n","   Total Images Analyzed: 1\n","   Structural Issues: 0\n","   Cosmetic Issues: 1\n","\n","‚ö†Ô∏è  SEVERITY BREAKDOWN:\n","   Critical: 0\n","   High: 0\n","   Medium/Low: 0\n","\n","================================================================================\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'image': 'beam crack 01.jfif',\n","  'result': {'classification': 'COSMETIC',\n","   'predicted_defect': 'concrete spalling with rebar exposure',\n","   'confidence': 0.7111527323722839,\n","   'severity': 'LOW',\n","   'all_predictions': [('concrete spalling with rebar exposure',\n","     0.7111527323722839),\n","    ('structural column damage', 0.1300022006034851),\n","    ('structural flexural crack', 0.05400834232568741),\n","    ('structural shear crack', 0.040456004440784454),\n","    ('short column defect', 0.0363411009311676),\n","    ('cosmetic surface issue', 0.0168857853859663),\n","    ('cosmetic plaster crack', 0.0075498525984585285),\n","    ('cosmetic paint damage', 0.0036039361730217934)]}}]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Upload images in Colab\n","from google.colab import files\n","\n","print(\"üì∏ Upload your crack/defect images:\")\n","uploaded = files.upload()\n","\n","# Analyze all uploaded images\n","image_paths = list(uploaded.keys())\n","analyzer.batch_analyze(image_paths)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"w0Sk6U-ngu2e","executionInfo":{"status":"ok","timestamp":1770323699434,"user_tz":-360,"elapsed":59369,"user":{"displayName":"Maharuj Mahadi","userId":"06320750487425513154"}},"outputId":"378080be-01be-4ff7-8b45-e2c461c6a7a3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["üì∏ Upload your crack/defect images:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-622454e0-c7d4-4ee5-83d6-8486d7778fc3\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-622454e0-c7d4-4ee5-83d6-8486d7778fc3\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Beam crack 02.jfif to Beam crack 02.jfif\n","\n","################################################################################\n","IMAGE 1/1\n","################################################################################\n","\n","================================================================================\n","STRUCTURAL DEFECT ANALYSIS\n","================================================================================\n","Image: Beam crack 02.jfif\n","\n","üîç Step 1: Analyzing image...\n","   General: there is a picture of a room with a ceiling that has been stripped\n","   Detailed: a detailed view of a ceiling with a hole in the ceiling\n","\n","üîç Step 2: Visual question answering...\n","   Q: Is this a crack?\n","   A: is this a crack?\n","   Q: Is the crack diagonal or horizontal?\n","   A: is the crack diagonal or horizontal?\n","   Q: Is there exposed rebar?\n","   A: is there exposed rebar?\n","   Q: Is there concrete spalling?\n","   A: is there concrete spalling?\n","   Q: Is this on a column or beam?\n","   A: is this on a column or beam?\n","   Q: Is the crack wide?\n","   A: is the crack wide?\n","   Q: Is this structural damage?\n","   A: is this structural damage?\n","\n","üîç Step 3: Classifying defect type...\n","\n","================================================================================\n","üìä ANALYSIS RESULTS\n","================================================================================\n","\n","üè∑Ô∏è  CLASSIFICATION: COSMETIC\n","‚ö†Ô∏è  SEVERITY LEVEL: LOW\n","üéØ PREDICTED DEFECT: concrete spalling with rebar exposure\n","üìà CONFIDENCE: 74.25%\n","\n","üìã DETAILED PREDICTIONS:\n","--------------------------------------------------------------------------------\n","üü¢ 74.25% - concrete spalling with rebar exposure\n","üî¥ 12.48% - structural column damage\n","üî¥ 4.25% - structural flexural crack\n","üî¥ 3.58% - structural shear crack\n","üü¢ 3.25% - short column defect\n","\n","üí° RECOMMENDATIONS:\n","--------------------------------------------------------------------------------\n","‚úÖ COSMETIC ISSUE:\n","   - Schedule routine maintenance\n","   - Cosmetic repair sufficient\n","   - Continue normal monitoring\n","\n","================================================================================\n","\n","\n","================================================================================\n","BATCH ANALYSIS SUMMARY REPORT\n","================================================================================\n","\n","üìä OVERVIEW:\n","   Total Images Analyzed: 1\n","   Structural Issues: 0\n","   Cosmetic Issues: 1\n","\n","‚ö†Ô∏è  SEVERITY BREAKDOWN:\n","   Critical: 0\n","   High: 0\n","   Medium/Low: 0\n","\n","================================================================================\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'image': 'Beam crack 02.jfif',\n","  'result': {'classification': 'COSMETIC',\n","   'predicted_defect': 'concrete spalling with rebar exposure',\n","   'confidence': 0.742529571056366,\n","   'severity': 'LOW',\n","   'all_predictions': [('concrete spalling with rebar exposure',\n","     0.742529571056366),\n","    ('structural column damage', 0.12477575242519379),\n","    ('structural flexural crack', 0.04245448112487793),\n","    ('structural shear crack', 0.03583596274256706),\n","    ('short column defect', 0.032543476670980453),\n","    ('cosmetic surface issue', 0.011910424567759037),\n","    ('cosmetic plaster crack', 0.005926030687987804),\n","    ('cosmetic paint damage', 0.004024311900138855)]}}]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Upload images in Colab\n","from google.colab import files\n","\n","print(\"üì∏ Upload your crack/defect images:\")\n","uploaded = files.upload()\n","\n","# Analyze all uploaded images\n","image_paths = list(uploaded.keys())\n","analyzer.batch_analyze(image_paths)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FQk2VOY5iMLG","executionInfo":{"status":"ok","timestamp":1770324085655,"user_tz":-360,"elapsed":58676,"user":{"displayName":"Maharuj Mahadi","userId":"06320750487425513154"}},"outputId":"c9086e49-2ccb-4e79-9830-bdc34812088a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["üì∏ Upload your crack/defect images:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-b33e3dc5-032f-469a-8cdc-25e63e67866e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-b33e3dc5-032f-469a-8cdc-25e63e67866e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving col ceack 3.jfif to col ceack 3.jfif\n","\n","################################################################################\n","IMAGE 1/1\n","################################################################################\n","\n","================================================================================\n","STRUCTURAL DEFECT ANALYSIS\n","================================================================================\n","Image: col ceack 3.jfif\n","\n","üîç Step 1: Analyzing image...\n","   General: arafed concrete column with a hole in it with a sign on it\n","   Detailed: a detailed view of a concrete pillar with a hole in it\n","\n","üîç Step 2: Visual question answering...\n","   Q: Is this a crack?\n","   A: is this a crack?\n","   Q: Is the crack diagonal or horizontal?\n","   A: is the crack diagonal or horizontal?\n","   Q: Is there exposed rebar?\n","   A: is there exposed rebar?\n","   Q: Is there concrete spalling?\n","   A: is there concrete spalling?\n","   Q: Is this on a column or beam?\n","   A: is this on a column or beam?\n","   Q: Is the crack wide?\n","   A: is the crack wide?\n","   Q: Is this structural damage?\n","   A: is this structural damage?\n","\n","üîç Step 3: Classifying defect type...\n","\n","================================================================================\n","üìä ANALYSIS RESULTS\n","================================================================================\n","\n","üè∑Ô∏è  CLASSIFICATION: COSMETIC\n","‚ö†Ô∏è  SEVERITY LEVEL: LOW\n","üéØ PREDICTED DEFECT: concrete spalling with rebar exposure\n","üìà CONFIDENCE: 65.97%\n","\n","üìã DETAILED PREDICTIONS:\n","--------------------------------------------------------------------------------\n","üü¢ 65.97% - concrete spalling with rebar exposure\n","üî¥ 25.51% - structural column damage\n","üî¥ 2.86% - structural flexural crack\n","üü¢ 2.43% - short column defect\n","üî¥ 2.05% - structural shear crack\n","\n","üí° RECOMMENDATIONS:\n","--------------------------------------------------------------------------------\n","‚úÖ COSMETIC ISSUE:\n","   - Schedule routine maintenance\n","   - Cosmetic repair sufficient\n","   - Continue normal monitoring\n","\n","================================================================================\n","\n","\n","================================================================================\n","BATCH ANALYSIS SUMMARY REPORT\n","================================================================================\n","\n","üìä OVERVIEW:\n","   Total Images Analyzed: 1\n","   Structural Issues: 0\n","   Cosmetic Issues: 1\n","\n","‚ö†Ô∏è  SEVERITY BREAKDOWN:\n","   Critical: 0\n","   High: 0\n","   Medium/Low: 0\n","\n","================================================================================\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'image': 'col ceack 3.jfif',\n","  'result': {'classification': 'COSMETIC',\n","   'predicted_defect': 'concrete spalling with rebar exposure',\n","   'confidence': 0.659702718257904,\n","   'severity': 'LOW',\n","   'all_predictions': [('concrete spalling with rebar exposure',\n","     0.659702718257904),\n","    ('structural column damage', 0.255109041929245),\n","    ('structural flexural crack', 0.02855474315583706),\n","    ('short column defect', 0.02432461455464363),\n","    ('structural shear crack', 0.020513826981186867),\n","    ('cosmetic surface issue', 0.0070011853240430355),\n","    ('cosmetic plaster crack', 0.0027464476879686117),\n","    ('cosmetic paint damage', 0.0020474521443247795)]}}]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Upload images in Colab\n","from google.colab import files\n","\n","print(\"üì∏ Upload your crack/defect images:\")\n","uploaded = files.upload()\n","\n","# Analyze all uploaded images\n","image_paths = list(uploaded.keys())\n","analyzer.batch_analyze(image_paths)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Bi7lOZjAhXoe","executionInfo":{"status":"ok","timestamp":1770323867327,"user_tz":-360,"elapsed":60555,"user":{"displayName":"Maharuj Mahadi","userId":"06320750487425513154"}},"outputId":"e3373115-0938-4403-d782-b2e75b7e171b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["üì∏ Upload your crack/defect images:\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-0bc129dc-45b9-4e10-a51f-a9a60e3e4349\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-0bc129dc-45b9-4e10-a51f-a9a60e3e4349\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving col crack 01.jfif to col crack 01.jfif\n","\n","################################################################################\n","IMAGE 1/1\n","################################################################################\n","\n","================================================================================\n","STRUCTURAL DEFECT ANALYSIS\n","================================================================================\n","Image: col crack 01.jfif\n","\n","üîç Step 1: Analyzing image...\n","   General: three pieces of stone with a carving of a man and a woman\n","   Detailed: a detailed view of a group of four stone sculptures on a white surface\n","\n","üîç Step 2: Visual question answering...\n","   Q: Is this a crack?\n","   A: is this a crack?\n","   Q: Is the crack diagonal or horizontal?\n","   A: is the crack diagonal or horizontal?\n","   Q: Is there exposed rebar?\n","   A: is there exposed rebar?\n","   Q: Is there concrete spalling?\n","   A: is there concrete spalling?\n","   Q: Is this on a column or beam?\n","   A: is this on a column or beam?\n","   Q: Is the crack wide?\n","   A: is the crack wide?\n","   Q: Is this structural damage?\n","   A: is this structural damage?\n","\n","üîç Step 3: Classifying defect type...\n","\n","================================================================================\n","üìä ANALYSIS RESULTS\n","================================================================================\n","\n","üè∑Ô∏è  CLASSIFICATION: COSMETIC\n","‚ö†Ô∏è  SEVERITY LEVEL: LOW\n","üéØ PREDICTED DEFECT: concrete spalling with rebar exposure\n","üìà CONFIDENCE: 64.83%\n","\n","üìã DETAILED PREDICTIONS:\n","--------------------------------------------------------------------------------\n","üü¢ 64.83% - concrete spalling with rebar exposure\n","üî¥ 16.71% - structural column damage\n","üî¥ 5.54% - structural flexural crack\n","üü¢ 4.75% - short column defect\n","üî¥ 4.18% - structural shear crack\n","\n","üí° RECOMMENDATIONS:\n","--------------------------------------------------------------------------------\n","‚úÖ COSMETIC ISSUE:\n","   - Schedule routine maintenance\n","   - Cosmetic repair sufficient\n","   - Continue normal monitoring\n","\n","================================================================================\n","\n","\n","================================================================================\n","BATCH ANALYSIS SUMMARY REPORT\n","================================================================================\n","\n","üìä OVERVIEW:\n","   Total Images Analyzed: 1\n","   Structural Issues: 0\n","   Cosmetic Issues: 1\n","\n","‚ö†Ô∏è  SEVERITY BREAKDOWN:\n","   Critical: 0\n","   High: 0\n","   Medium/Low: 0\n","\n","================================================================================\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'image': 'col crack 01.jfif',\n","  'result': {'classification': 'COSMETIC',\n","   'predicted_defect': 'concrete spalling with rebar exposure',\n","   'confidence': 0.6482559442520142,\n","   'severity': 'LOW',\n","   'all_predictions': [('concrete spalling with rebar exposure',\n","     0.6482559442520142),\n","    ('structural column damage', 0.1671174168586731),\n","    ('structural flexural crack', 0.05535547062754631),\n","    ('short column defect', 0.04753004387021065),\n","    ('structural shear crack', 0.04175886884331703),\n","    ('cosmetic surface issue', 0.0233489777892828),\n","    ('cosmetic plaster crack', 0.009973129257559776),\n","    ('cosmetic paint damage', 0.006660132203251123)]}}]"]},"metadata":{},"execution_count":7}]}]}